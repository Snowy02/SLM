Of course. I have analyzed the code you provided in the screenshots and found the exact cause of the issue.

The problem is that your handle_nlp_query function is not structured as a proper Gradio generator. A generator function uses the yield keyword to pass back intermediate results. Gradio is designed to show the progress bar when a generator function starts yielding and remove it only when the function completes (i.e., it stops yielding).

Your current code has a yield statement, but it is not being used correctly, and the function's structure doesn't follow the required pattern. This prevents Gradio from knowing when the task is "finished," so it never removes the old progress bar.

The Problem Lines in Your Code

Based on your screenshots, these are the areas causing the issue:

Incorrect yield Structure: You are yielding a tuple of many variables (nlp_textbox, chatbot_output, etc.) at the end of the else block. The structure and number of yielded items must exactly match the outputs defined in your .click() or .submit() event listener.

Missing yield for Other Paths: The if intent == "dependency": block does not have a yield statement, so it doesn't update the UI correctly.

No Immediate yield: The function does not yield an update right after receiving the user's query. This is crucial for showing the user's prompt in the chat and starting the progress bar before the heavy processing begins.

The Fix

I will correct your handle_nlp_query function to implement the proper generator pattern. This will solve the duplicate progress bar problem permanently.

Here is the corrected handle_nlp_query function. Replace your entire existing handle_nlp_query function with this one.

Generated python
# --- START: FIX FOR DUPLICATE PROGRESS BARS ---
# Explanation: This entire function is being replaced to implement the correct generator pattern.
# A generator uses `yield` to return intermediate steps. This allows Gradio to properly
# manage the progress bar, showing it at the start and removing it upon completion.

def handle_nlp_query(question, history, progress=gr.Progress(track_tqdm=True)):
    """
    Handles the NLP query as a generator to support progress bars correctly.
    """
    # --- FIX 1: Handle empty input and immediately exit the generator. ---
    # This ensures no processing happens for an empty query.
    if not question or not question.strip():
        gr.Warning("Please enter a question before running the query.")
        return "", history, history, gr.update() # Use return to stop the generator

    # --- FIX 2: Immediately yield the user's message with a placeholder. ---
    # This makes the UI feel responsive. The user sees their message appear instantly,
    # and this first `yield` triggers Gradio to show the progress bar.
    history.append([question, None])
    yield "", history, history, gr.update(visible=True)

    bot_response = ""
    try:
        progress(0, desc="Identifying Intent...")
        res = query_handler.identify_intent(question)
        res = res.strip('`').strip('json')
        if not res.strip():
            raise ValueError("Received an empty response from identify_intent")
        response_json = json.loads(res)
        intent = response_json.get("intent")
    except (json.JSONDecodeError, ValueError) as e:
        error_message = f"Failed to parse response from the language model: {e}"
        bot_response = f"<p style='color:red;'>Error: Could not understand the model's response. Details: {html.escape(str(e))}</p>"
        # --- FIX 3: Update history with the error and yield the final result. ---
        # The generator function finishes here for the error case.
        history[-1][1] = bot_response
        yield "", history, history, gr.update(visible=True)
        return # Stop the generator

    if intent == "dependency":
        progress(0.3, desc="Generating & running Cypher query...")
        result_data = query_handler.run_query(question)
        final_answer = result_data.get("result")
        df_markdown = ""
        if isinstance(final_answer, list) and final_answer:
            df = pd.DataFrame(final_answer)
            df_markdown = "### Query Result\n" + df.to_markdown(index=False) + "\n\n<hr>"

        intermediate_html = "<h3>Query Execution Details</h3>"
        intermediate_html += f"<b>Original Question:</b><p><i>{html.escape(result_data.get('question', 'N/A'))}</i></p>"
        for step in result_data.get('intermediate_steps', []):
            status_color = 'green' if step['status'] == 'Success' else 'red'
            intermediate_html += f"<hr><b>Attempt {step['attempt']}: <span style='color:{status_color};'>{step['status']}</span></b>"
            intermediate_html += f"<pre><code class='language-cypher'>{html.escape(step['cypher_query'])}</code></pre>"
            if step['status'] != 'Success':
                intermediate_html += f"<p style='color:red;'><b>Error:</b> {html.escape(str(step['error']))}</p>"
        if not df_markdown:
             intermediate_html += "<hr><b>Final answer:</b><p>Query executed successfully but returned no results.</p>"
        bot_response = df_markdown + intermediate_html
    else: # Assumes 'code_explanation' intent
        progress(0.3, desc="Extracting entities...")
        entities_str = query_handler.extract_entities(question)
        entities_arr = [e.strip() for e in entities_str.split(",") if e.strip()]
        
        progress(0.6, desc="Querying graph for entities...")
        query = f"WITH {json.dumps(entities_arr)} AS names MATCH (n) WHERE n.name IN names RETURN n"
        result = query_handler.extract_result_for_query(query=query)
        
        explanation_html = "<h3>Code Explanations</h3>"
        if not result:
            explanation_html += "<p>Could not find any matching code elements for the entities found.</p>"
        else:
            total_items = len(result)
            for i, item in enumerate(result):
                # --- FIX 4: Update the main progress bar instead of using a nested one. ---
                loop_progress = 0.6 + (0.4 * ((i + 1) / total_items))
                progress(loop_progress, desc=f"Generating explanation {i+1}/{total_items}...")
                
                node = item.get("n", {})
                if 'source' in node and 'name' in node:
                    code_explanation = query_handler.explain_code(node["source"])
                    escaped_source = html.escape(node["source"])
                    explanation_html += f"<hr><h4>Explanation for: <code>{html.escape(node['name'])}</code></h4>"
                    explanation_html += f"<pre><code>{escaped_source}</code></pre>"
                    explanation_html += f"<div>{code_explanation}</div>"
        bot_response = explanation_html

    # --- FIX 5: Update the final placeholder in history with the complete bot response. ---
    history[-1][1] = bot_response

    # --- FIX 6: Yield the final state. ---
    # This is the last `yield`. After this, the function finishes, and Gradio knows
    # to remove the progress bar from the UI.
    yield "", history, history, gr.update(visible=True)
# --- END: FIX FOR DUPLICATE PROGRESS BARS ---


To make this work, ensure your event listeners (.click and .submit) are correct:

Your event listeners at the bottom of the script should look exactly like this, passing the correct inputs and outputs.

Generated python
nlp_textbox.submit(
        fn=handle_nlp_query,
        inputs=[nlp_textbox, chatbot_history_state],
        outputs=[nlp_textbox, chatbot_output, chatbot_history_state, nlp_results_col]
    )
    nlp_run_button.click(
        fn=handle_nlp_query,
        inputs=[nlp_textbox, chatbot_history_state],
        outputs=[nlp_textbox, chatbot_output, chatbot_history_state, nlp_results_col]
    )
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END
