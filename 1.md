You are right. I was only giving you separate pieces. My apologies for the piecemeal approach. Let's integrate the content from the USAGE.md file directly into the main README.md, making it a single, comprehensive document.

Here's the updated README.md file with the usage guide integrated:

Generated markdown
# Source Codebase Analytics

[![Build Status](<insert CI/CD badge URL here>)](<link to CI/CD pipeline>)
[![Coverage Status](<insert coverage badge URL here>)](<link to coverage report>)

**Bridging the Knowledge Gap in Enterprise Applications**

## Table of Contents

*   [1. Introduction](#1-introduction)
*   [2. Features](#2-features)
*   [3. Technologies Used](#3-technologies-used)
*   [4. Prerequisites](#4-prerequisites)
*   [5. Installation](#5-installation)
*   [6. Configuration](#6-configuration)
    *   [6.1 Environment Variables](#61-environment-variables)
*   [7. Usage](#7-usage)
    *   [7.1 Generating Semantic Models](#71-generating-semantic-models)
    *   [7.2 Parsing Stored Procedures](#72-parsing-stored-procedures)
    *   [7.3 Extracting Table Schemas](#73-extracting-table-schemas)
    *   [7.4 Building the Neo4j Knowledge Graph](#74-building-the-neo4j-knowledge-graph)
    *   [7.5 Querying the Knowledge Graph](#75-querying-the-knowledge-graph)
*   [8. Architecture](#8-architecture)
*   [9. Deployment](#9-deployment)
*   [10. Contributing](#10-contributing)
*   [11. Security](#11-security)
*   [12. Support](#12-support)
*   [13. Contact](#13-contact)

---

## 1. Introduction

Enterprise applications often suffer from information silos, with valuable knowledge scattered across disparate GitHub repositories, SharePoint documents, and other sources. This project aims to bridge this knowledge gap by providing a centralized, queryable knowledge graph of the codebase. It automatically analyzes C# and SQL codebases, extracts semantic information, and constructs a Neo4j knowledge graph. The system can then be queried using natural language questions through the Prodigy API, allowing developers to quickly understand the structure, dependencies, and behavior of the applications.

## 2. Features

*   **Automated Codebase Analysis:** Analyzes C# and SQL codebases to extract semantic information, including classes, methods, relationships, constants, and enums.
*   **Knowledge Graph Generation:** Constructs a Neo4j knowledge graph representing codebase structure, dependencies, and relationships.
*   **Natural Language Query Interface:** Uses the Prodigy API to enable querying the knowledge graph with natural language questions.
*   **Dependency Analysis:** Identifies dependencies between repositories, classes, methods, and stored procedures.
*   **Stored Procedure Analysis:** Extracts information about SQL Server stored procedures, including tables used, parameters, and source code.
*   **Table Schema Extraction:** Extracts the schema and index information of tables from SQL Server.
*   **Class Diagram Generation:** Generates class diagrams to visualize the structure of classes and their relationships (potentially using Mermaid.js).
*   **Code Explanation:** Explains the functionality of code snippets using the Prodigy API.
*   **Intent Identification:** Identifies the intent of user queries (e.g., dependency analysis, code explanation, optimization).

## 3. Technologies Used

*   C# (.NET 9.0): Used for the code analyzer (`analyzer.exe` and `StoredProcedureParser.exe`).
*   Python 3.9: Used for graph building, API interaction, data transformation, and web interface.
*   Neo4j: Used as the graph database to store the knowledge graph.
*   Prodigy API: Used as the LLM service for natural language query processing.
*   pyodbc: Used for connecting to and querying SQL Server databases.
*   Rapidfuzz: Used for fuzzy matching of entities in user queries.
*   Gradio: Used for the user interface.

## 4. Prerequisites

*   .NET 9.0 Runtime: Required to run the C# code analyzer.  Download from [https://dotnet.microsoft.com/en-us/download/dotnet/9.0](https://dotnet.microsoft.com/en-us/download/dotnet/9.0).
*   Python 3.9: With the packages specified in `requirements.txt`.  Use `pip install -r requirements.txt`.
*   Neo4j: A running instance of Neo4j. Download and install from [https://neo4j.com/download/](https://neo4j.com/download/). Ensure it is running on `bolt://localhost:7687` with default credentials (or update the `.env` file accordingly).
*   SQL Server: Access to the SQL Server database containing the stored procedures.  Ensure you have the `ODBC Driver 17 for SQL Server` installed.
*   Prodigy API Credentials:  Valid API key, App ID, App Key, and Resource ID for the Prodigy API.

## 5. Installation

1.  **Clone the repository:**

    ```bash
    git clone <repository_url>
    cd <project_directory>
    ```

2.  **Install Python dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure Environment Variables:**  See the [Configuration](#6-configuration) section.

4.  **Build the .NET analyzers:**

    *   Open the `sre-codebase-analytics\ASTGenerators\DotNetParser\analyzer.sln` and `sre-codebase-analytics\StoredProcedureParser\StoredProcedureParser.sln` solutions in Visual Studio.
    *   Build them in Release mode, targeting .NET 9.0.
    *   The resulting executables (`analyzer.exe` and `StoredProcedureParser.exe`) will be located in the `bin\Release\net9.0\` directories of their respective projects.

## 6. Configuration

### 6.1 Environment Variables

The project uses environment variables for configuration. Create a `.env` file in the root directory of the project, based on the `.env.example` file.  **Do not commit your `.env` file to the repository!**  Add `.env` to your `.gitignore` file.

The following environment variables are required:

*   `NEO4J_URI`: The URI of the Neo4j instance (e.g., `bolt://localhost:7687`).
*   `NEO4J_USER`: The username for connecting to Neo4j (e.g., `neo4j`).
*   `NEO4J_PASSWORD`: The password for connecting to Neo4j.  **Important:** Use a strong, unique password.
*   `PRODIGY_API_BASE_URL`: The base URL for the Prodigy API (e.g., `https://ait.studiogateway.chubb.com/enterprise.operations.prodigy-core-services`).
*   `PRODIGY_APP_ID`:  The App ID for the Prodigy API.
*   `PRODIGY_APP_KEY`:  The App Key for the Prodigy API.
*   `PRODIGY_RESOURCE`:  The Resource ID for the Prodigy API.
*   `SQL_SERVER`: The SQL Server instance (e.g., `server.excelminds.com`).
*   `SQL_DATABASE`: The SQL Server database (e.g., `sconners`).

Here's an example `.env.example` file:

```properties
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password
PRODIGY_API_BASE_URL=https://ait.studiogateway.chubb.com/enterprise.operations.prodigy-core-services
PRODIGY_APP_ID=your_prodigy_app_id
PRODIGY_APP_KEY=your_prodigy_app_key
PRODIGY_RESOURCE=your_prodigy_resource_id
SQL_SERVER=server.excelminds.com
SQL_DATABASE=sconners

7. Usage

The following sections provide step-by-step instructions on how to use the Source Codebase Analytics system. Ensure that you have completed all the steps in the Prerequisites and Installation sections before proceeding.

7.1 Generating Semantic Models

Update repo_list.txt: Open the repo_list.txt file and add the absolute paths to the GitHub repositories you want to analyze. Each path should be on a new line. For example:

Generated code
C:\Users\CHMUKTH\Documents\GitHub\repository_name1
C:\Users\CHMUKTH\Documents\GitHub\repository_name2
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END

Important: Use absolute paths.

Run generate_model.py: Execute the generate_model.py script:

Generated bash
python generate_model.py
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

This will generate semantic models for each repository in repo_list.txt. The models will be stored as <repository_name>_model.json files in the semantic_models directory.

Troubleshooting:

If you encounter an error, check the console output for error messages.

Ensure that the paths in repo_list.txt are correct.

Ensure that the .NET analyzers (analyzer.exe) are built and available in the specified location.

7.2 Parsing Stored Procedures

Update parse_sql.py: Open the parse_sql.py file and update the FOLDERS list with the absolute paths to the directories containing the SQL stored procedure files:

Generated python
FOLDERS = [
    r"C:\Users\TUSAURA\Documents\GitHub\Database\Chubb.Marketplace.Database.SCDINS\dbfrontend\Stored Procedures",
    r"C:\Users\TUSAURA\Documents\GitHub\Database\Chubb.Marketplace.Database.SCDINS\dbatpdm\Stored Procedures",
    r"C:\Users\TUSAURA\Documents\GitHub\Database\Chubb.Marketplace.Database.SCDINS\dbbpm\Stored Procedures"
]
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Important: Use absolute paths.

Run parse_sql.py: Execute the parse_sql.py script:

Generated bash
python parse_sql.py
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

This will parse the stored procedures and generate JSON models. The models will be stored as <stored_procedure_name>_model.json files in the stored_procs directory.

Troubleshooting:

If you encounter an error, check the console output for error messages.

Ensure that the paths in the FOLDERS list are correct.

Ensure that the .NET parser (StoredProcedureParser.exe) is built and available in the specified location.

7.3 Extracting Table Schemas

Update table_extract.py: Open the table_extract.py file and update the server and database variables with the connection details for your SQL Server database:

Generated python
server = "server.excelminds.com"
database = "sconners"
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Run table_extract.py: Execute the table_extract.py script:

Generated bash
python table_extract.py
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

This will extract the schema and index information for tables in the database. The table information will be stored as <table_name>.json files in the table_jsons directory.

Troubleshooting:

If you encounter an error, check the console output for error messages.

Ensure that the connection details in table_extract.py are correct.

Ensure that the user specified in the Trusted_Connection of the connection string has the necessary permissions to query the database schema.

7.4 Building the Neo4j Knowledge Graph

Run graph_builder.py: Execute the graph_builder.py script:

Generated bash
python graph_builder.py
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

The script will prompt you: Do you want to clear the entire graph before building? (y/n):

If this is the first time you are running the script, type y and press Enter to clear the existing graph.

If you have already built the graph and want to add more information to it without deleting the existing data, type n and press Enter.

This will build the Neo4j knowledge graph from the semantic models in the semantic_models directory.

Run graph_builder_stored_proc.py: Execute the graph_builder_stored_proc.py script:

Generated bash
python graph_builder_stored_proc.py
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

This will add stored procedure information to the Neo4j knowledge graph.

Troubleshooting:

If you encounter an error, check the console output for error messages.

Ensure that Neo4j is running and that the connection details in the .env file are correct.

Ensure that the semantic_models, stored_procs, and table_jsons directories contain the generated JSON models.

7.5 Querying the Knowledge Graph

Start the Web Application: Execute the web.py script:

Generated bash
python web.py
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

This will start the web application on http://localhost:5000 (or a different address, as specified in web.py).

Open the Web Interface: Open your web browser and navigate to the address shown in console output.

Enter a Query: In the text box, enter a natural language question about the codebase, such as:

"What classes depend on the PaymentService class?"

"What stored procedures are called by the CustomerRepository class?"

"List all methods in the UserService class."

View the Results: The system will use the Prodigy API to generate a Cypher query, execute it against the Neo4j knowledge graph, and display the results in an HTML table.

Troubleshooting:

If you encounter an error, check the console output for error messages.

Ensure that the Prodigy API credentials in the .env file are correct.

Ensure that the Neo4j database is running and accessible.

If the query returns no results, try rephrasing the question or checking the knowledge graph for the relevant information.

8. Architecture

[Insert Architecture Diagram Here]

Generated mermaid
graph LR
    A[GitHub Repositories] --> B((.NET Analyzer));
    C[SQL Server Database] --> D((Stored Procedure Parser));
    B --> E[semantic_models];
    D --> F[stored_procs];
    G[table_jsons] --> H((graph_builder.py & graph_builder_stored_proc.py));
    E --> H;
    F --> H;
    H --> I((Neo4j Graph Database));
    J((Web Application)) --> K((graph_query_handler.py));
    I --> K;
    K --> L((Prodigy API));
    L --> J;
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Mermaid
IGNORE_WHEN_COPYING_END

The above diagram illustrates the flow of data from the code repositories and SQL Server database to the Neo4j knowledge graph, and then to the user via the Prodigy API.

Key Components:

GitHub Repositories: Source code repositories containing C# code. Analyzed by the .NET Analyzer.

SQL Server Database: Contains stored procedures and table definitions. Parsed by the Stored Procedure Parser and table_extract.py.

.NET Analyzer (analyzer.exe): Analyzes C# code to extract semantic information and generates JSON models in the semantic_models directory.

Stored Procedure Parser (StoredProcedureParser.exe): Parses SQL stored procedures to extract information and generates JSON models in the stored_procs directory.

table_extract.py: Extracts schema and index information from the SQL Server database and generates JSON files in the table_jsons directory.

generate_model.py: Orchestrates the execution of the .NET Analyzer for each repository.

graph_builder.py: Builds the Neo4j knowledge graph from the semantic models in the semantic_models directory.

graph_builder_stored_proc.py: Adds stored procedure information to the Neo4j knowledge graph.

Neo4j Graph Database: Stores the knowledge graph representing the structure and dependencies of the codebase.

graph_query_handler.py: Handles natural language queries, translates them into Cypher queries using the Prodigy API, and executes the queries against the Neo4j knowledge graph.

Prodigy API: Provides natural language processing capabilities for translating user questions into Cypher queries.

web.py: A web application built with Gradio providing a user interface for querying the knowledge graph.

9. Deployment

Describe the deployment environments (development, staging, production). This information is company-specific.

Explain how to deploy the different components: This information is company-specific.

.NET Analyzers: These need to be built and the executables deployed to a location accessible by the Python scripts (or included in the repository). Consider a shared network drive or artifact repository.

Python Scripts: These can be deployed to a server or cloud environment with the necessary dependencies installed. Consider using a virtual environment manager (e.g., venv or conda). Containerization (e.g., Docker) is recommended for consistent deployments.

Neo4j: This should be deployed as a managed service (e.g., Neo4j AuraDB) or on a dedicated server. Ensure proper backups and monitoring are configured.

Outline the CI/CD pipeline for automated deployments. Describe the steps involved in building, testing, and deploying the application. Link to the CI/CD configuration files (e.g., Jenkinsfile, .gitlab-ci.yml, GitHub Actions workflow).

10. Contributing

See CONTRIBUTING.md for details. (Create this file and include your company's contributing guidelines.)

11. Security

Never commit .env files containing real secrets to the repository. Ensure .env is in your .gitignore file.

Use environment variables for all sensitive information: Neo4j credentials, Prodigy API keys, SQL Server credentials.

Store environment variables securely: Use a secrets management system (e.g., HashiCorp Vault, AWS Secrets Manager, Azure Key Vault) for sensitive deployments.

Ensure proper access control to the Neo4j database and SQL Server database. Follow the principle of least privilege.

Validate user input: Sanitize user input to prevent Cypher injection attacks.

Follow the company's security policies for handling sensitive data and accessing internal systems. Contact the security team for guidance.

12. Support

Contact the [Your Team Name] team at your_team_email@example.com for support. Use the internal issue tracker for bug reports and feature requests.

13. Contact

[Your Name] - your.email@example.com

[Team Name] - [Link to team's internal page or contact form]

To Complete This README

Fill in the placeholders: Replace the <...> placeholders with the actual values for your project. Pay special attention to the Deployment section.

Create the architecture diagram: This is crucial for understanding the system. Consider using Mermaid.js or draw.io. I have provided a Mermaid diagram as an example.

Create the CONTRIBUTING.md file: Follow the company's contribution guidelines.

Create a requirements.txt file: List all the Python dependencies. Use pip freeze > requirements.txt in your environment, then carefully review and edit the file to remove any unnecessary dependencies or development tools.

Review and enhance the Security section: Consult with your security team to ensure all relevant security considerations are addressed.

Test the installation and usage instructions: Make sure they are accurate and easy to follow on a clean machine.

Review with your team: Get feedback from your colleagues to ensure the README is clear, complete, and accurate.

Ensure .gitignore is uptodate.

Generated code
This is now a complete `README.md` file, including the usage instructions. I strongly advise you to review it thoroughly and fill in all the missing details. This should give you a solid and professional final product.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END
