Okay, I apologize for that oversight. You're right; the Usage section I provided in the full README.md template was not in a user-friendly, runnable README format. I'll correct that now with a version tailored for actual execution. This will be a separate document, not part of the main README, to be used as a hands-on guide. This would often be named USAGE.md or QUICKSTART.md.

Here's a USAGE.md file you can create to provide detailed, actionable steps for using the Source Codebase Analytics system:

Generated markdown
# Usage Guide - Source Codebase Analytics

This guide provides step-by-step instructions on how to use the Source Codebase Analytics system to analyze codebases, build a knowledge graph, and query it using natural language.

## Prerequisites

Before you begin, make sure you have completed the following steps:

1.  **Installed the Prerequisites:** See the [Prerequisites](#4-prerequisites) section of the main `README.md` file for details.
2.  **Installed the Python Dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

3.  **Configured Environment Variables:** Create a `.env` file in the project root directory and set the required environment variables as described in the [Configuration](#6-configuration) section of the `README.md` file.
4.  **Built the .NET Analyzers:** Build the `analyzer.sln` and `StoredProcedureParser.sln` solutions in Visual Studio in Release mode, targeting .NET 9.0.

## Step-by-Step Instructions

### 1. Generate Semantic Models

1.  **Update `repo_list.txt`:** Open the `repo_list.txt` file and add the *absolute* paths to the GitHub repositories you want to analyze. Each path should be on a new line. For example:

    ```
    C:\Users\CHMUKTH\Documents\GitHub\repository_name1
    C:\Users\CHMUKTH\Documents\GitHub\repository_name2
    ```

    **Important:** Use *absolute* paths.

2.  **Run `generate_model.py`:** Execute the `generate_model.py` script:

    ```bash
    python generate_model.py
    ```

    This will generate semantic models for each repository in `repo_list.txt`. The models will be stored as `<repository_name>_model.json` files in the `semantic_models` directory.

    **Troubleshooting:**

    *   If you encounter an error, check the console output for error messages.
    *   Ensure that the paths in `repo_list.txt` are correct.
    *   Ensure that the .NET analyzers (`analyzer.exe`) are built and available in the specified location.

### 2. Parse Stored Procedures

1.  **Update `parse_sql.py`:** Open the `parse_sql.py` file and update the `FOLDERS` list with the *absolute* paths to the directories containing the SQL stored procedure files:

    ```python
    FOLDERS = [
        r"C:\Users\TUSAURA\Documents\GitHub\Database\Chubb.Marketplace.Database.SCDINS\dbfrontend\Stored Procedures",
        r"C:\Users\TUSAURA\Documents\GitHub\Database\Chubb.Marketplace.Database.SCDINS\dbatpdm\Stored Procedures",
        r"C:\Users\TUSAURA\Documents\GitHub\Database\Chubb.Marketplace.Database.SCDINS\dbbpm\Stored Procedures"
    ]
    ```

    **Important:** Use *absolute* paths.

2.  **Run `parse_sql.py`:** Execute the `parse_sql.py` script:

    ```bash
    python parse_sql.py
    ```

    This will parse the stored procedures and generate JSON models. The models will be stored as `<stored_procedure_name>_model.json` files in the `stored_procs` directory.

    **Troubleshooting:**

    *   If you encounter an error, check the console output for error messages.
    *   Ensure that the paths in the `FOLDERS` list are correct.
    *   Ensure that the .NET parser (`StoredProcedureParser.exe`) is built and available in the specified location.

### 3. Extract Table Schemas

1.  **Update `table_extract.py`:** Open the `table_extract.py` file and update the `server` and `database` variables with the connection details for your SQL Server database:

    ```python
    server = "server.excelminds.com"
    database = "sconners"
    ```

2.  **Run `table_extract.py`:** Execute the `table_extract.py` script:

    ```bash
    python table_extract.py
    ```

    This will extract the schema and index information for tables in the database. The table information will be stored as `<table_name>.json` files in the `table_jsons` directory.

    **Troubleshooting:**

    *   If you encounter an error, check the console output for error messages.
    *   Ensure that the connection details in `table_extract.py` are correct.
    *   Ensure that the user specified in the `Trusted_Connection` of the connection string has the necessary permissions to query the database schema.

### 4. Build the Neo4j Knowledge Graph

1.  **Run `graph_builder.py`:** Execute the `graph_builder.py` script:

    ```bash
    python graph_builder.py
    ```

    The script will prompt you:  `Do you want to clear the entire graph before building? (y/n):`

    *   If this is the *first* time you are running the script, type `y` and press Enter to clear the existing graph.
    *   If you have already built the graph and want to add more information to it without deleting the existing data, type `n` and press Enter.

    This will build the Neo4j knowledge graph from the semantic models in the `semantic_models` directory.

2.  **Run `graph_builder_stored_proc.py`:** Execute the `graph_builder_stored_proc.py` script:

    ```bash
    python graph_builder_stored_proc.py
    ```

    This will add stored procedure information to the Neo4j knowledge graph.

    **Troubleshooting:**

    *   If you encounter an error, check the console output for error messages.
    *   Ensure that Neo4j is running and that the connection details in the `.env` file are correct.
    *   Ensure that the `semantic_models`, `stored_procs`, and `table_jsons` directories contain the generated JSON models.

### 5. Query the Knowledge Graph

1.  **Start the Web Application:** Execute the `web.py` script:

    ```bash
    python web.py
    ```

    This will start the web application on `http://localhost:5000` (or a different address, as specified in `web.py`).

2.  **Open the Web Interface:** Open your web browser and navigate to the address shown in console output.

3.  **Enter a Query:** In the text box, enter a natural language question about the codebase, such as:

    *   "What classes depend on the PaymentService class?"
    *   "What stored procedures are called by the CustomerRepository class?"
    *   "List all methods in the UserService class."

4.  **View the Results:** The system will use the Prodigy API to generate a Cypher query, execute it against the Neo4j knowledge graph, and display the results in an HTML table.

    **Troubleshooting:**

    *   If you encounter an error, check the console output for error messages.
    *   Ensure that the Prodigy API credentials in the `.env` file are correct.
    *   Ensure that the Neo4j database is running and accessible.
    *   If the query returns no results, try rephrasing the question or checking the knowledge graph for the relevant information.

## Example Workflow

Here's an example of how to use the Source Codebase Analytics system to analyze a new repository:

1.  Add the path to the repository to the `repo_list.txt` file.
2.  Run `generate_model.py` to generate a semantic model for the repository.
3.  Run `graph_builder.py` to add the repository information to the Neo4j knowledge graph.
4.  Open the web interface and query the knowledge graph to explore the structure and dependencies of the repository.

This `USAGE.md` file provides a much more practical guide for users to get started with your system. Remember to test these instructions yourself to make sure they work as expected. Also, adapt them to your specific user base and workflows.
